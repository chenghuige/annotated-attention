{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as ds\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.babi import BABI20, BABI20Field\n",
    "import torchtext.data\n",
    "torch.device.index\n",
    "train, val, test = BABI20.iters(batch_size=20, task = 1, device=torch.device(0))\n",
    "QUERY = train.dataset.fields[\"query\"]\n",
    "STORY = train.dataset.fields[\"story\"]\n",
    "ANSWER = train.dataset.fields[\"answer\"]\n",
    "def wrap(data):\n",
    "    return ((batch.story, \n",
    "            batch.query,\n",
    "            batch.answer.squeeze(1)) for batch in  data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_instance(x, x_tilde, y):\n",
    "    print(\"Correct answer: \", ANSWER.vocab.itos[y])\n",
    "    for w in x_tilde:\n",
    "        print(\"%20s \"%(QUERY.vocab.itos[w]), end = \" \")\n",
    "    print()\n",
    "    #for i, py in enumerate(p_y.probs):\n",
    "    #    print(\"%20s %f\"%(ANSWER.vocab.itos[i], py), end= \" \")\n",
    "    #print()\n",
    "    for i, s in enumerate(x):\n",
    "        for k, w in enumerate(s):\n",
    "            if STORY.vocab.itos[w] != \"<pad>\":\n",
    "                print(\"%10s\"%(STORY.vocab.itos[w]), end=\" \")\n",
    "            elif k == 0:\n",
    "                break\n",
    "        if k > 0:\n",
    "            print(\"%f \"%p.probs[i].data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  bedroom\n",
      "               Where                    is                Daniel  \n",
      "    Daniel  journeyed         to        the    bedroom "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d33ee0637b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-f124e408f6a3>\u001b[0m in \u001b[0;36mdisplay_instance\u001b[0;34m(x, x_tilde, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%f \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "x, x_tilde, y = next(wrap(val))\n",
    "display_instance(x[0], x_tilde[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"Simple implementation of attention.\"\n",
    "    def __init__(self, value_encoder, query_encoder, \n",
    "                 evidence_encoder=None, combiner=None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.value_encoder = value_encoder\n",
    "        self.query_encoder = query_encoder\n",
    "        \n",
    "        # If we have acess to the evidence (inference network)\n",
    "        self.evidence_encoder = evidence_encoder\n",
    "        self.combiner = combiner\n",
    "        \n",
    "    def forward(self, src, query, answer=None):\n",
    "        value_vecs = self.value_encoder(src)\n",
    "        query_vecs = self.query_encoder(query)\n",
    "        # Incorporate evidence.\n",
    "        if self.answer_encoder is not None:\n",
    "            answer_vecs = self.answer_encoder(answer)\n",
    "            query_vecs = self.combiner(torch.cat([query_vecs, answer_vecs], -1))\n",
    "        \n",
    "        # Apply attention.\n",
    "        attention_logits = torch.bmm(src_vecs, query_vecs.transpose(1, 2)).squeeze(-1)\n",
    "        return value_vecs, attention_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"A simple LSTM text encoder.\"\n",
    "    def __init__(self, vocab_size, hidden_size, last_only=False, pad=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab_size, hidden_size, padding_idx=pad) \n",
    "        self.encoder = nn.LSTM(hidden_size=hidden_size,\n",
    "                               input_size=hidden_size,\n",
    "                               bidirectional=not last_only) \n",
    "        self.last_only = last_only\n",
    "        \n",
    "    def forward(self, text):\n",
    "        x = self.lut(text)\n",
    "        # If there is an extra dimension, sum it out.\n",
    "        if x.dim() >= 4:\n",
    "            x = x.sum(2)\n",
    "        x, _ = self.encoder(x)\n",
    "        if self.last_only:\n",
    "            return x[:, -1:]\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (alignment): Attention(\n",
       "    (src_encoder): Encoder(\n",
       "      (lut): Embedding(20, 50, padding_idx=0)\n",
       "      (encoder): LSTM(50, 50, bidirectional=True)\n",
       "    )\n",
       "    (query_encoder): Encoder(\n",
       "      (lut): Embedding(20, 100, padding_idx=0)\n",
       "      (encoder): LSTM(100, 100)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (prediction_network): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (inference): Attention(\n",
       "    (src_encoder): Encoder(\n",
       "      (lut): Embedding(20, 50, padding_idx=0)\n",
       "      (encoder): LSTM(50, 50, bidirectional=True)\n",
       "    )\n",
       "    (query_encoder): Encoder(\n",
       "      (lut): Embedding(20, 100, padding_idx=0)\n",
       "      (encoder): LSTM(100, 100)\n",
       "    )\n",
       "    (answer_encoder): Encoder(\n",
       "      (lut): Embedding(20, 100, padding_idx=0)\n",
       "      (encoder): LSTM(100, 100)\n",
       "    )\n",
       "    (combiner): Sequential(\n",
       "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Module()\n",
    "model.alignment = Attention(Encoder(len(STORY.vocab), 50, pad=STORY.vocab.stoi[\"pad\"]),\n",
    "                            Encoder(len(QUERY.vocab), 100, last_only=True, \n",
    "                                    pad=STORY.vocab.stoi[\"pad\"])\n",
    "                           )\n",
    "model.generator =  nn.Sequential(\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, len(ANSWER.vocab))) \n",
    "\n",
    "model.inference = Attention(Encoder(len(STORY.vocab), 50, pad=STORY.vocab.stoi[\"pad\"]),\n",
    "                            Encoder(len(QUERY.vocab), 100, last_only=True, \n",
    "                                    pad=STORY.vocab.stoi[\"pad\"]),\n",
    "                            Encoder(len(ANSWER.vocab), 100, last_only=True, \n",
    "                                    pad=ANSWER.vocab.stoi[\"pad\"]),\n",
    "                            nn.Sequential(\n",
    "                                nn.Linear(200, 100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100, 100)\n",
    "                            )\n",
    "                           )\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(p, v): \n",
    "    return torch.bmm(p.unsqueeze(1), v)\n",
    "\n",
    "def KL(p, q): \n",
    "    return ds.kl_divergence(p, q)\n",
    "\n",
    "def Cat1(logits):\n",
    "    return ds.OneHotCategorical(logits=logits)\n",
    "\n",
    "def Cat(logits): \n",
    "    return ds.Categorical(logits=logits)\n",
    "\n",
    "def CatP(probs):\n",
    "    return ds.Categorical(probs=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft(gen, x, x_tilde, y, p, v):\n",
    "    context = E(p.probs, v)\n",
    "    logits = gen(context).squeeze(1)\n",
    "    p_y = Cat(logits)\n",
    "    return -p_y.log_prob(y), p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard(gen, x, x_tilde, y, p, v):\n",
    "    choice = p.sample()\n",
    "    context = E(choice, v)\n",
    "    p_y = Cat(gen(context).squeeze(1))\n",
    "    reward = p_y.log_prob(y).detach()\n",
    "    return -(p_y.log_prob(y) + p.log_prob(choice) * reward), p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_soft(gen, x, x_tilde, y, p, v):\n",
    "    choice = p.sample()\n",
    "    context = E(choice, v)\n",
    "    p_y = Cat(gen(context).squeeze(1))\n",
    "    nbaseline, p_y_soft = soft(gen, x, x_tilde, y, p, v)\n",
    "    reward = (p_y.log_prob(y) - (-nbaseline)).detach()\n",
    "    return -(p_y_soft.log_prob(y) + p_y.log_prob(y) + p.log_prob(choice) * reward), p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum(gen, x, x_tilde, y, p, v):\n",
    "    logits = model.generator(v)\n",
    "    p_y = CatP(probs=E(p.probs, Cat(logits).probs).squeeze(1))\n",
    "    return -p_y.log_prob(y), p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae(gen, x, x_tilde, y, p, v):\n",
    "    _, logits = model.inference(x, x_tilde, y.unsqueeze(1))\n",
    "    q = Cat1(logits)\n",
    "    choice = q.sample()\n",
    "    context = E(choice, v)\n",
    "    p_y = Cat(gen(context).squeeze(1))\n",
    "    nbaseline, p_y_soft = soft(gen, x, x_tilde, y, p, v)\n",
    "    reward = (p_y.log_prob(y) - (-nbaseline)).detach()\n",
    "    return -(p_y_soft.log_prob(y) + p_y.log_prob(y) + q.log_prob(choice) * reward - KL(q, p)), p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8999223709106445 0.15000000596046448\n",
      "1.7574795484542847 0.15000000596046448\n",
      "1.7703651189804077 0.15000000596046448\n",
      "1.764735221862793 0.15000000596046448\n",
      "1.7493127584457397 0.15000000596046448\n",
      "1.7248371839523315 0.20000000298023224\n",
      "1.7008472681045532 0.3499999940395355\n",
      "0.6060455441474915 0.8500000238418579\n",
      "0.5408384203910828 0.9000000357627869\n",
      "0.01800537109375 1.0\n",
      "0.007454657461494207 1.0\n",
      "0.0031062127090990543 1.0\n",
      "0.002271890640258789 1.0\n",
      "0.001722145127132535 1.0\n",
      "0.0015566826332360506 1.0\n",
      "0.0031032562255859375 1.0\n",
      "0.0009783267742022872 1.0\n",
      "0.004041910171508789 1.0\n",
      "0.0007668972248211503 1.0\n",
      "0.0005860328674316406 1.0\n",
      "0.0004953384632244706 1.0\n",
      "0.0008023261907510459 1.0\n",
      "0.0007991790771484375 1.0\n",
      "0.0009588241809979081 1.0\n",
      "0.0009348392486572266 1.0\n",
      "0.0007482051732949913 1.0\n",
      "0.0023152113426476717 1.0\n",
      "0.0025069713592529297 1.0\n",
      "0.0010557174682617188 1.0\n",
      "0.0006608963012695312 1.0\n",
      "0.0005763531080447137 1.0\n",
      "0.0005115509266033769 1.0\n",
      "0.0003913402615580708 1.0\n",
      "0.0002566814364399761 1.0\n",
      "0.00020303727069403976 1.0\n",
      "0.00016050339036155492 1.0\n",
      "0.00012311936006881297 1.0\n",
      "9.069442603504285e-05 1.0\n",
      "6.742477853549644e-05 1.0\n",
      "3.466606358415447e-05 1.0\n",
      "2.994537317135837e-05 1.0\n",
      "2.2649765014648438e-05 1.0\n",
      "0.0059656621888279915 1.0\n",
      "0.010755443945527077 1.0\n",
      "0.0031919479370117188 1.0\n",
      "0.001379251480102539 1.0\n",
      "0.002253079554066062 1.0\n",
      "0.0004554748593363911 1.0\n",
      "0.0004321813757997006 1.0\n",
      "0.0003129243850708008 1.0\n",
      "0.00024497509002685547 1.0\n",
      "0.00020196438708808273 1.0\n",
      "0.00017204285541083664 1.0\n",
      "0.00014963150897528976 1.0\n",
      "0.00013217926607467234 1.0\n",
      "0.00011830330186057836 1.0\n",
      "0.00010657310485839844 1.0\n",
      "9.722710092319176e-05 1.0\n",
      "8.897781663108617e-05 1.0\n",
      "8.20159912109375e-05 1.0\n",
      "7.596016075694934e-05 1.0\n",
      "7.042884681141004e-05 1.0\n",
      "6.551742990268394e-05 1.0\n",
      "6.108284287620336e-05 1.0\n",
      "0.00012750625319313258 1.0\n",
      "0.0001029014601954259 1.0\n",
      "8.268356759799644e-05 1.0\n",
      "6.937980651855469e-05 1.0\n",
      "6.09397902735509e-05 1.0\n",
      "5.33103957423009e-05 1.0\n",
      "4.787445141118951e-05 1.0\n",
      "5.6934357417048886e-05 1.0\n",
      "0.002452349755913019 1.0\n",
      "0.0003981351910624653 1.0\n",
      "0.00011069775064243004 1.0\n",
      "5.519390106201172e-05 1.0\n",
      "2.3317337763728574e-05 1.0\n",
      "1.3971329281048384e-05 1.0\n",
      "9.918213436321821e-06 1.0\n",
      "7.867813110351562e-06 1.0\n",
      "6.4373016357421875e-06 1.0\n",
      "4.339218321547378e-06 1.0\n",
      "2.6226043701171875e-06 1.0\n",
      "1.9073486328125e-06 1.0\n",
      "1.5258789289873675e-06 1.0\n",
      "1.3351440202313825e-06 1.0\n",
      "1.1920928955078125e-06 1.0\n",
      "1.144409225162235e-06 1.0\n",
      "1.0490417707842425e-06 1.0\n",
      "9.5367431640625e-07 1.0\n",
      "8.1062319168268e-07 1.0\n",
      "6.198883397701138e-07 1.0\n",
      "6.198883397701138e-07 1.0\n",
      "4.76837158203125e-07 1.0\n",
      "4.76837158203125e-07 1.0\n",
      "4.2915345943583816e-07 1.0\n",
      "3.814697322468419e-07 1.0\n",
      "3.814697322468419e-07 1.0\n",
      "3.814697322468419e-07 1.0\n",
      "3.337860050578456e-07 1.0\n"
     ]
    }
   ],
   "source": [
    "# opt = torch.optim.SGD(model.parameters(), lr=5)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "method = vae\n",
    "for epoch in range(100):\n",
    "    for x, x_tilde, y in wrap(test):\n",
    "        opt.zero_grad()\n",
    "        v, theta = model.alignment.forward(x, x_tilde)\n",
    "        p = Cat1(theta)\n",
    "        obj, p_y = method(model.generator, x, x_tilde, y, p, v)\n",
    "        obj = obj.mean()\n",
    "        obj.backward()\n",
    "        opt.step()\n",
    "        _, y_hat = p_y.probs.max(1)\n",
    "        correct = (y_hat == y).data.float().mean()\n",
    "    print(-p_y.log_prob(y).mean().detach().item(), correct.item() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = vae\n",
    "x, x_tilde, y = next(wrap(test))\n",
    "v, theta = model.alignment.forward(x, x_tilde)\n",
    "p = Cat1(theta)\n",
    "loss, p_y = method(model.generator, x, x_tilde, y, p, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer:  hallway\n",
      "               Where                    is                  John  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 1.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "      Mary  journeyed         to        the   bathroom 0.000002 \n",
      "      John  travelled         to        the    hallway 0.999973 \n",
      "Correct answer:  bathroom\n",
      "               Where                    is                  Mary  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000002                 back 0.000000               garden 0.000000               office 0.000000              kitchen 0.000000             bathroom 0.999997                Where 0.000000                   is 0.000000 \n",
      "      John      moved         to        the    bedroom 0.003428 \n",
      "    Daniel       went       back         to        the   bathroom 0.000764 \n",
      "      Mary  journeyed         to        the   bathroom 0.995678 \n",
      "      John  travelled         to        the    hallway 0.000101 \n",
      "Correct answer:  kitchen\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 1.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "    Sandra  journeyed         to        the    kitchen 0.999910 \n",
      "      John       went         to        the    hallway 0.000087 \n",
      "      John      moved         to        the    bedroom 0.000000 \n",
      "    Daniel       went       back         to        the   bathroom 0.000000 \n",
      "      Mary  journeyed         to        the   bathroom 0.000000 \n",
      "      John  travelled         to        the    hallway 0.000003 \n",
      "Correct answer:  hallway\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 1.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "      John       went         to        the     garden 0.000010 \n",
      "    Sandra  travelled         to        the    hallway 0.999958 \n",
      "    Sandra  journeyed         to        the    kitchen 0.000032 \n",
      "      John       went         to        the    hallway 0.000000 \n",
      "      John      moved         to        the    bedroom 0.000000 \n",
      "    Daniel       went       back         to        the   bathroom 0.000000 \n",
      "      Mary  journeyed         to        the   bathroom 0.000000 \n",
      "      John  travelled         to        the    hallway 0.000000 \n",
      "Correct answer:  kitchen\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 1.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "    Sandra      moved         to        the    kitchen 0.997874 \n",
      "    Sandra       went       back         to        the   bathroom 0.001899 \n",
      "      John       went         to        the     garden 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.000227 \n",
      "    Sandra  journeyed         to        the    kitchen 0.000000 \n",
      "      John       went         to        the    hallway 0.000000 \n",
      "      John      moved         to        the    bedroom 0.000000 \n",
      "    Daniel       went       back         to        the   bathroom 0.000000 \n",
      "      Mary  journeyed         to        the   bathroom 0.000000 \n",
      "      John  travelled         to        the    hallway 0.000000 \n",
      "Correct answer:  hallway\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 1.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.999275 \n",
      "    Sandra  travelled         to        the    kitchen 0.000725 \n",
      "Correct answer:  garden\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 1.000000               office 0.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "    Sandra      moved         to        the     garden 0.998656 \n",
      "      Mary       went         to        the   bathroom 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.001331 \n",
      "    Sandra  travelled         to        the    kitchen 0.000013 \n",
      "Correct answer:  hallway\n",
      "               Where                    is                Daniel  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 1.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 0.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "    Daniel  journeyed         to        the    hallway 1.000000 \n",
      "    Sandra  travelled         to        the     office 0.000000 \n",
      "    Sandra      moved         to        the     garden 0.000000 \n",
      "      Mary       went         to        the   bathroom 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.000000 \n",
      "    Sandra  travelled         to        the    kitchen 0.000000 \n",
      "Correct answer:  office\n",
      "               Where                    is                Sandra  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 1.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "      John      moved         to        the    hallway 0.000030 \n",
      "    Daniel  journeyed         to        the     office 0.000025 \n",
      "    Daniel  journeyed         to        the    hallway 0.000002 \n",
      "    Sandra  travelled         to        the     office 0.999715 \n",
      "    Sandra      moved         to        the     garden 0.000218 \n",
      "      Mary       went         to        the   bathroom 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.000002 \n",
      "    Sandra  travelled         to        the    kitchen 0.000008 \n",
      "Correct answer:  office\n",
      "               Where                    is                Daniel  \n",
      "               <pad> 0.000000                  the 0.000000                   to 0.000000                 went 0.000000               Sandra 0.000000                 Mary 0.000000                 John 0.000000               Daniel 0.000000              hallway 0.000000            journeyed 0.000000                moved 0.000000            travelled 0.000000              bedroom 0.000000                 back 0.000000               garden 0.000000               office 1.000000              kitchen 0.000000             bathroom 0.000000                Where 0.000000                   is 0.000000 \n",
      "      John  journeyed         to        the     office 0.000534 \n",
      "      John  travelled         to        the   bathroom 0.000051 \n",
      "      John      moved         to        the    hallway 0.000017 \n",
      "    Daniel  journeyed         to        the     office 0.999236 \n",
      "    Daniel  journeyed         to        the    hallway 0.000161 \n",
      "    Sandra  travelled         to        the     office 0.000000 \n",
      "    Sandra      moved         to        the     garden 0.000000 \n",
      "      Mary       went         to        the   bathroom 0.000000 \n",
      "    Sandra  travelled         to        the    hallway 0.000000 \n",
      "    Sandra  travelled         to        the    kitchen 0.000000 \n"
     ]
    }
   ],
   "source": [
    "for j in range(0,10):\n",
    "    print(\"Correct answer: \", ANSWER.vocab.itos[y[j] ])\n",
    "    for w in x_tilde[j]:\n",
    "        print(\"%20s \"%(QUERY.vocab.itos[w]), end = \" \")\n",
    "    print()\n",
    "    for i, py in enumerate(p_y.probs[j]):\n",
    "        print(\"%20s %f\"%(ANSWER.vocab.itos[i], py), end= \" \")\n",
    "    print()\n",
    "    for i, s in enumerate(x[j]):\n",
    "        for k, w in enumerate(s):\n",
    "            if STORY.vocab.itos[w] != \"<pad>\":\n",
    "                print(\"%10s\"%(STORY.vocab.itos[w]), end=\" \")\n",
    "            elif k == 0:\n",
    "                break\n",
    "        if k > 0:\n",
    "            print(\"%f \"%p.probs[j, i].data.item())\n",
    "        \n",
    "        \n",
    "    #p.probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.Categorical(logits=torch.autograd.Variable(torch.Tensor([0.2, 0.8]))\n",
    "              ).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-5fc19e5a66ca>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-5fc19e5a66ca>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ds.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.Categorical()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
